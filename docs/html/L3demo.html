
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>L3demo</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-10-19"><meta name="DC.source" content="L3demo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Train an L^3 camera</a></li><li><a href="#3">Load scene to capture with camera</a></li><li><a href="#4">Perform optics and sensor simulation</a></li><li><a href="#5">Move deeper in the code to more L^3 specific functions</a></li><li><a href="#6">Show CFA and RAW image</a></li><li><a href="#7">Show Patch Types</a></li><li><a href="#8">Load and show patches from RAW image</a></li><li><a href="#9">Find patch luminance for each patch</a></li><li><a href="#10">Find closest patch luminance from trained set for each patch</a></li><li><a href="#11">Global Linear Pipeline  (simpler alternative to full L^3 pipeline)</a></li><li><a href="#12">Divide patches into flat and texture</a></li><li><a href="#13">1.  Calculate the mean in each color channel</a></li><li><a href="#14">2.  Subtract the mean from each pixel with the corresponding color</a></li><li><a href="#15">3.  Find patch contrast by summing 0 mean patch with abs value</a></li><li><a href="#16">4.  Compare patch contrast to threshold.  If contrast&lt;threshold, patch is flat.  If contrast&gt;threshold, patch is texture.</a></li><li><a href="#17">Apply filters to flat patches</a></li><li><a href="#18">Flip texture patches into canonical form</a></li><li><a href="#19">Cluster texture patches</a></li><li><a href="#20">Apply filters to texture patches</a></li><li><a href="#21">Calculate final output images</a></li><li><a href="#22">Show which of the trained patch luminance values used at each pixel</a></li><li><a href="#23">Calculate ideal XYZ image</a></li><li><a href="#24">Crop black border from all images</a></li><li><a href="#25">Scale lrgb result images</a></li><li><a href="#26">Convert lrgb to srgb</a></li><li><a href="#27">Show the sRGB results</a></li><li><a href="#28">Default ISET pipeline result</a></li></ul></div><pre class="codeinput"><span class="comment">% The following script explains how the L^3 pipeline is applied to a RAW</span>
<span class="comment">% image from a sensor with an arbitrary CFA pattern.</span>

<span class="comment">% The results of the training process, such as filters, are shown.  The</span>
<span class="comment">% calculations required to perform the training are not described but can</span>
<span class="comment">% be viewed in L3Train.m.</span>

<span class="comment">% The following code is taken from s_L3render and the functions that are</span>
<span class="comment">% subsequently called.  There have been slight modifications as needed</span>
<span class="comment">% and the plotting code has been added.  Indentations correspond with which</span>
<span class="comment">% m files the code was taken from.</span>
</pre><h2>Train an L^3 camera<a name="2"></a></h2><pre class="codeinput">s_L3TrainCamera
clearvars <span class="string">**</span> <span class="string">-except</span> <span class="string">camera</span>  <span class="comment">%clear workspace except for camera</span>
</pre><h2>Load scene to capture with camera<a name="3"></a></h2><pre class="codeinput">scene = sceneFromFile(<span class="string">'StuffedAnimals_tungsten-hdrs'</span>,<span class="string">'multispectral'</span>);
meanLuminance = 600;
fovScene      = 10;
scene = sceneSet(scene,<span class="string">'hfov'</span>,fovScene);
scene = sceneAdjustLuminance(scene,meanLuminance);

<span class="comment">% Adjust FOV of camera to match scene</span>
camera = cameraSet(camera,<span class="string">'sensor fov'</span>,fovScene);
</pre><pre class="codeoutput">Reading multispectral data with mcCOEF.
Saved using svd method
</pre><h2>Perform optics and sensor simulation<a name="4"></a></h2><p>Following is in cameraCompute, which is called using the following line: camera = cameraCompute(camera,scene);</p><pre class="codeinput">  oi     = cameraGet(camera,<span class="string">'oi'</span>);
  sensor = cameraGet(camera,<span class="string">'sensor'</span>);
  vci    = cameraGet(camera,<span class="string">'vci'</span>);

  oi     = oiCompute(oi,scene);
  sensor = sensorCompute(sensor,oi);
</pre><h2>Move deeper in the code to more L^3 specific functions<a name="5"></a></h2><p>Following is in vcimageCompute, which is called using the followign line:</p><pre class="codeinput"><span class="comment">% vci    = vcimageCompute(vci,sensor);</span>
    L3 = imageGet(vci,<span class="string">'L3'</span>);

<span class="comment">%   Following is in L3render, which is called using the following line:</span>
    <span class="comment">% [resultL3,lumIdx] = L3render(L3,sensor,'local');</span>
</pre><h2>Show CFA and RAW image<a name="6"></a></h2><p>The CFA pattern is visible in the RAW image.  Here the RAW image is drawn as a monochrome image although the measurement at each pixel has an associated color.</p><p>The 2x2 RGBW CFA is used in this example.</p><pre class="codeinput">      <span class="comment">% RAW sensor image</span>
      inputIm = sensorGet(sensor,<span class="string">'volts'</span>);
      vcNewGraphWin; imagesc(inputIm/max(inputIm(:))); colormap(gray)
      sz      = sensorGet(sensor,<span class="string">'size'</span>);
      title(<span class="string">'RAW sensor image'</span>)

      <span class="comment">% RGBW CFA</span>
      L3plot(L3,<span class="string">'cfa full'</span>);       title(<span class="string">'RGBW CFA'</span>)

      <span class="comment">% Spectral sensitivities</span>
      wave = sensorGet(sensor,<span class="string">'wave'</span>);
      sensitivities = sensorGet(sensor,<span class="string">'spectral QE'</span>);
      figure;   hold <span class="string">on</span>
      plotcolors=<span class="string">'rgbk'</span>;
      <span class="keyword">for</span> colornum=1:4
          plot(wave,sensitivities(:,colornum),plotcolors(colornum))
      <span class="keyword">end</span>
      xlabel(<span class="string">'Wavelength (nm)'</span>);  title(<span class="string">'Spectral Sensitivities'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_01.png" alt=""> <img vspace="5" hspace="5" src="L3demo_02.png" alt=""> <img vspace="5" hspace="5" src="L3demo_03.png" alt=""> <h2>Show Patch Types<a name="7"></a></h2><p>L^3 is a patch based algorithm.  The output at each pixel is a function only of a set of nearby pixels in the input RAW image.</p><p>Here a patch is a 5x5 square of pixels centered at the pixel where we want to calculate the output.  Since the CFA is a 2x2 pattern, there are 4 types of pixels (RGB and W), which could be at the center of a patch.  Therefore, we have the following four types of patches.</p><p>Different filters have been trained for each patch type.  We will build the output image by iterating through each patch type and using the appropriate filters.</p><pre class="codeinput">      warning(<span class="string">'off'</span>) <span class="comment">%#ok&lt;*WNOFF&gt;   %needed because of CFA/data mismatch</span>
      <span class="keyword">for</span> patchtyperow=1:2
          <span class="keyword">for</span> patchtypecol=1:2
              L3plot(L3,<span class="string">'block pattern'</span>,[patchtyperow,patchtypecol]);
              title([<span class="string">'Patch Type '</span>,num2str(patchtyperow),<span class="string">', '</span><span class="keyword">...</span>
                  ,num2str(patchtypecol)])
          <span class="keyword">end</span>
      <span class="keyword">end</span>
      warning(<span class="string">'on'</span>) <span class="comment">%#ok&lt;*WNON&gt;</span>
</pre><img vspace="5" hspace="5" src="L3demo_04.png" alt=""> <img vspace="5" hspace="5" src="L3demo_05.png" alt=""> <img vspace="5" hspace="5" src="L3demo_06.png" alt=""> <img vspace="5" hspace="5" src="L3demo_07.png" alt=""> <h2>Load and show patches from RAW image<a name="8"></a></h2><p>All patches of a particular patch type are loaded at the same time.</p><pre class="codeinput">      <span class="comment">% For rest of this article, just consider patch type 1,1 that has a</span>
      <span class="comment">% center R pixel.  Normally all patch types are looped over.</span>
      rr=1; cc=1;
      L3 = L3Set(L3,<span class="string">'patch type'</span>,[rr,cc]);

      <span class="comment">% Load all patches of this cfa position from RAW image</span>
      inputPatches = L3sensor2Patches(L3,inputIm);

      <span class="comment">% Patches are stored as a matrix where each vectorized patch is</span>
      <span class="comment">% stored in a row.</span>

      <span class="comment">% Size of this matrix: number of pixels in a patch x number of</span>
      <span class="comment">% R pixels in the RAW image.  (The border of the image is ignored</span>
      <span class="comment">% when a patch cannot fit so this number is slightly off.)</span>
      patchesSize = size(inputPatches)

      indices=ceil(rand(1,25)*size(inputPatches,2)/2); <span class="comment">%randomly pick 25</span>

      L3 = L3Set(L3,<span class="string">'sensor patches'</span>, inputPatches);
      L3plotpatches(L3,indices,5,5);
      title(<span class="string">'Some of the Input Patches of Type 1,1'</span>)
</pre><pre class="codeoutput">
patchesSize =

          25       11934

</pre><img vspace="5" hspace="5" src="L3demo_08.png" alt=""> <h2>Find patch luminance for each patch<a name="9"></a></h2><p>Since light level significantly alters the amount of noise in the measurements, the L^3 algorithm adapts to the local light level.</p><p>The local light level for patch is called its patch luminance.  The patch luminance is a weighted average of the RAW voltage values and is calculated by:       1.  Finding the average color across the patch for each CFA color       2.  Averaging the numbers in (1). This calculation is performed by applying the luminance filter using an inner product.</p><pre class="codeinput"><span class="comment">%     Following is in L3applyPipeline2Patches, which is called using:</span>
<span class="comment">%      [xhatL3,lumIdx] = L3applyPipeline2Patches(L3,inputPatches,L3Type);</span>

        luminancefilter = L3Get(L3,<span class="string">'luminance filter'</span>);
        L3plot(L3,<span class="string">'luminance filter'</span>);   title(<span class="string">'Luminance Filter'</span>)

        allPatches = L3Get(L3,<span class="string">'sensor patches'</span>);
        patchluminances = luminancefilter*allPatches;

        figure;     hist(patchluminances,50)
        xlabel(<span class="string">'Patch Luminance'</span>);      ylabel(<span class="string">'Number of Patches'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_09.png" alt=""> <img vspace="5" hspace="5" src="L3demo_10.png" alt=""> <h2>Find closest patch luminance from trained set for each patch<a name="10"></a></h2><p>Filters are learned for a predefined set of patch luminance values.  For each patch, calculate the patch luminance and find the closest luminance value that was used for training. the patch</p><pre class="codeinput">        <span class="comment">%List of patch luminance values used for training</span>
        patchLuminanceSamples = L3Get(L3,<span class="string">'luminance list'</span>)

        differences = repmat(patchluminances',1,length(patchLuminanceSamples)) - <span class="keyword">...</span>
            repmat(patchLuminanceSamples,length(patchluminances),1);
        [~,luminanceindex] = min(abs(differences'));

        figure;     hist(patchLuminanceSamples(luminanceindex),50)
        xlabel(<span class="string">'Closest Patch Luminance'</span>);      ylabel(<span class="string">'Number of Patches'</span>)

        <span class="comment">% For rest of this article, just consider the patches that are</span>
        <span class="comment">% closest to the first trained patch luminance value.</span>
        <span class="comment">% Normally all patch types are looped over.</span>
        ll = 1;
        currentpatches = find(luminanceindex == ll);

        <span class="comment">%Set current patch luminance index</span>
        L3 = L3Set(L3,<span class="string">'luminance type'</span>,ll);
        L3 = L3Set(L3,<span class="string">'sensor patches'</span>, allPatches(:,currentpatches));
</pre><pre class="codeoutput">
patchLuminanceSamples =

  Columns 1 through 6

    0.1800    0.3100    0.4400    0.5700    0.7000    0.8300

  Columns 7 through 10

    0.9600    1.0900    1.2200    1.3500

</pre><img vspace="5" hspace="5" src="L3demo_11.png" alt=""> <h2>Global Linear Pipeline  (simpler alternative to full L^3 pipeline)<a name="11"></a></h2><p>The simplest way to run the pipeline is to have a single filter for each patch type and luminance level.  We call this the global linear pipeline (which is a bad name).  This pipeline is simpler than the full L^3 pipelien and performance is usually only a little worse.</p><pre class="codeinput">        L3plot(L3,<span class="string">'global filter'</span>);
        subplot(1,3,2)
        title(<span class="string">'Global Pipeline Filters (3 plots are for XYZ)'</span>)

        <span class="comment">%Output for global linear pipeline is calculated using a single</span>
        <span class="comment">%multiplication.</span>
        globalpipelinefilter = L3Get(L3,<span class="string">'global filter'</span>);
        xhatL3(:,currentpatches) = globalpipelinefilter* L3Get(L3,<span class="string">'sensor patches'</span>);
</pre><img vspace="5" hspace="5" src="L3demo_12.png" alt=""> <h2>Divide patches into flat and texture<a name="12"></a></h2><p>Patches are divided into two groups, flat and texture.</p><p>Flat patches come from uniform regions of an image.  For a flat patch all measurements of the same color are nearly equal.</p><p>Texture patches come from edges or texture regions of an imae.  For a texture patch, there is more variation across the patch measurements.</p><pre class="codeinput"><span class="comment">% Following are all implemented by repeated calls to L3Get.  Each of</span>
<span class="comment">% the following steps are calculated for each patch separately.</span>
</pre><h2>1.  Calculate the mean in each color channel<a name="13"></a></h2><pre class="codeinput">            <span class="comment">% means = L3Get(L3,'sensor patch means');</span>
            meansFilter = L3Get(L3,<span class="string">'means filter'</span>);
            patches     = L3Get(L3,<span class="string">'sensor patches'</span>);
            means = meansFilter*patches;

            L3plot(L3,<span class="string">'mean filter'</span>);
            subplot(1,4,2)
            title(<span class="string">'Color Channel Means Filters (4 plots are for RGBW)'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_13.png" alt=""> <h2>2.  Subtract the mean from each pixel with the corresponding color<a name="14"></a></h2><pre class="codeinput">            <span class="comment">% patcheszeromean = L3Get(L3,'sensor patch zero mean')</span>
            blockPattern = L3Get(L3,<span class="string">'block pattern'</span>);
            patcheszeromean = L3adjustpatchmean(patches,-means,blockPattern);
</pre><h2>3.  Find patch contrast by summing 0 mean patch with abs value<a name="15"></a></h2><pre class="codeinput">            <span class="comment">% contrasts = L3Get(L3,'sensor patch contrasts');</span>
            contrasts = mean(abs(patcheszeromean));
</pre><h2>4.  Compare patch contrast to threshold.  If contrast&lt;threshold, patch is flat.  If contrast&gt;threshold, patch is texture.<a name="16"></a></h2><pre class="codeinput">            <span class="comment">% flatindices = L3Get(L3,'flat indices');</span>
            flatThreshold = L3Get(L3,<span class="string">'flat threshold'</span>);
            flatindices = (flatThreshold &gt;= contrasts);
</pre><h2>Apply filters to flat patches<a name="17"></a></h2><p>Optimal filters learned for the flat patches are applied to get the output XYZ estimates.</p><pre class="codeinput">        flatfilters = L3Get(L3,<span class="string">'flat filters'</span>);
        xhatL3(:,currentpatches(flatindices)) = L3applyfilters(L3,flatfilters,flatindices);

        L3plot(L3,<span class="string">'flat filter'</span>);
        subplot(1,3,2)
        title(<span class="string">'Filters for Flat Patches (3 plots are for XYZ)'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_14.png" alt=""> <h2>Flip texture patches into canonical form<a name="18"></a></h2><p>Since there are significant spatial differences within texture patches, it helps (a little) to decrease the possible variation in the patches. The patches are flipped over the vertical, horizontal, and main diagonal (assuming there is symmetry in the CFA pattern across these directions) so that each flipped texture patch has higher averages in the top, left, and above diagonal halves.</p><pre class="codeinput">      textureindices = find(L3Get(L3,<span class="string">'texture indices'</span>));
      randompick=ceil(rand(1,25)*size(textureindices,2)/2); <span class="comment">%randomly pick 25</span>
      selectedtextureindices = textureindices(randompick);

      L3plotpatches(L3,selectedtextureindices,5,5);
      title(<span class="string">'Some Texture Patches before flip'</span>)

      L3 = L3flippatches(L3);

      L3plotpatches(L3,selectedtextureindices,5,5);
      title(<span class="string">'Some Texture Patches after flip'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_15.png" alt=""> <img vspace="5" hspace="5" src="L3demo_16.png" alt=""> <h2>Cluster texture patches<a name="19"></a></h2><p>The texture patches can be further subdivided using a hierarchical clustering method.  The idea is that by subdividing, similar patches will be grouped together and optimal filters for each cluster will be able to exploit the similar statistics of the patches.  This offers only small improvements for general natural scenes.</p><pre class="codeinput">        L3 = L3clustertexturepatches(L3);
</pre><h2>Apply filters to texture patches<a name="20"></a></h2><p>Optimal filters learned for each cluster of texture patches are applied to get the output XYZ estimates.</p><pre class="codeinput">        texturefilters = L3Get(L3,<span class="string">'texture filters'</span>);
        clustermembers = L3Get(L3,<span class="string">'cluster members'</span>);
        treedepth      = L3Get(L3,<span class="string">'tree depth'</span>);
        numclusters    = L3Get(L3,<span class="string">'nclusters'</span>);
        clusterrange   = 1:numclusters; <span class="comment">%this should probably just be leaves</span>
        <span class="keyword">for</span> clusternum = clusterrange
            <span class="comment">%clusterindices is vector of length equal to the number of allPatches,</span>
            <span class="comment">%each entry is 1 for each patch in the current cluster and 0 otherwise</span>
            clusterindices = <span class="keyword">...</span>
                floor(clustermembers/2^(treedepth-floor(log2(clusternum))-1))==clusternum;

            xhatL3(:,currentpatches(clusterindices)) = <span class="keyword">...</span>
                L3applyfilters(L3,texturefilters{clusternum},clusterindices);
        <span class="keyword">end</span>

        textureType = 1;
        L3Plot(L3,<span class="string">'texture filter'</span>,[rr,cc],ll,textureType);
        subplot(1,3,2)
        title(<span class="string">'Filters for all Texture Patches (3 plots are for XYZ)'</span>)

        textureType = 5;
        L3Plot(L3,<span class="string">'texture filter'</span>,[rr,cc],ll,textureType);
        subplot(1,3,2)
        title(<span class="string">'Filters for 1/4 of Texture Patches (3 plots are for XYZ)'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_17.png" alt=""> <img vspace="5" hspace="5" src="L3demo_18.png" alt=""> <h2>Calculate final output images<a name="21"></a></h2><p>Once the above calculations are performed for each patch type and patch luminance (which happens by calling cameraCompute), we can get the final output images.</p><pre class="codeinput"><span class="comment">% Calculate L^3 result</span>
[camera, lrgbL3] = cameraCompute(camera,scene);

<span class="comment">% Calculate global L^3 result</span>
camera  = cameraSet(camera,<span class="string">'vci name'</span>,<span class="string">'L3 global'</span>);
[camera, lrgbGlobal] = cameraCompute(camera,<span class="string">'sensor'</span>);
</pre><h2>Show which of the trained patch luminance values used at each pixel<a name="22"></a></h2><pre class="codeinput">vci = cameraGet(camera,<span class="string">'vci'</span>);
L3 = imageGet(vci,<span class="string">'L3'</span>);

lumIdx = L3Get(L3,<span class="string">'luminance index'</span>);
figure;  imagesc(lumIdx); axis <span class="string">image</span>; axis <span class="string">off</span>;
colorbar
title(<span class="string">'Luminance Value Used'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_19.png" alt=""> <h2>Calculate ideal XYZ image<a name="23"></a></h2><p>Following XYZ image is the result that would occur with no noise if we had a sensor that measured the XYZ channels at every pixel.</p><pre class="codeinput">[camera,xyzIdeal] = cameraCompute(camera,scene,<span class="string">'idealxyz'</span>);
xyzIdeal = xyzIdeal/max(xyzIdeal(:));   <span class="comment">%scale to full display range</span>

figure;  image(xyzIdeal); axis <span class="string">image</span>; axis <span class="string">off</span>;
title(<span class="string">'Ideal XYZ'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_20.png" alt=""> <h2>Crop black border from all images<a name="24"></a></h2><p>L^3 does not give estimates for pixels near the image border.  An estimate is not possible for pixels when there is not enough room to fit a patch.</p><pre class="codeinput">xyzIdeal   = L3imcrop(L3,xyzIdeal);
lrgbL3      = L3imcrop(L3,lrgbL3);
lrgbGlobal  = L3imcrop(L3,lrgbGlobal);
</pre><h2>Scale lrgb result images<a name="25"></a></h2><p>To achieve consistent appearance for display, the result images are scaled so they have the same mean as the lrgbIdeal image.  This is more consistent across the images than scaling each independently to achieve a maximum value of 1.</p><pre class="codeinput">[srgbIdeal, lrgbIdeal] = xyz2srgb(xyzIdeal);
lrgbL3       = lrgbL3 * mean(lrgbIdeal(:)) / mean(lrgbL3(:));
lrgbGlobal   = lrgbGlobal * mean(lrgbIdeal(:)) / mean(lrgbGlobal(:));
</pre><h2>Convert lrgb to srgb<a name="26"></a></h2><pre class="codeinput">srgbL3      = lrgb2srgb(ieClip(lrgbL3,0,1));
srgbGlobal  = lrgb2srgb(ieClip(lrgbGlobal,0,1));
</pre><h2>Show the sRGB results<a name="27"></a></h2><pre class="codeinput">vcNewGraphWin;  imagesc(srgbIdeal); axis <span class="string">image</span>
title(<span class="string">'Ideal'</span>)

vcNewGraphWin; imagesc(srgbL3); axis <span class="string">image</span>
title(<span class="string">'L3'</span>)

vcNewGraphWin; imagesc(srgbGlobal); axis <span class="string">image</span>
title(<span class="string">'L3 Global'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_21.png" alt=""> <img vspace="5" hspace="5" src="L3demo_22.png" alt=""> <img vspace="5" hspace="5" src="L3demo_23.png" alt=""> <h2>Default ISET pipeline result<a name="28"></a></h2><p>Following is the default pipeline in ISET.  The settings and scaling are probably not set up properly.</p><pre class="codeinput">camera = cameraSet(camera,<span class="string">'vci name'</span>,<span class="string">'default'</span>);
[camera, lrgbbilinear] = cameraCompute(camera,<span class="string">'sensor'</span>);

<span class="comment">%Crop image to compare with other cropped images</span>
lrgbbilinear   = L3imcrop(L3,lrgbbilinear);

<span class="comment">% Scale and convert to srgb</span>
lrgbbilinear   = lrgbbilinear * mean(lrgbIdeal(:)) / mean(lrgbbilinear(:));
srgbbilinear   = lrgb2srgb(ieClip(lrgbbilinear,0,1));

vcNewGraphWin; imagesc(srgbbilinear); axis <span class="string">image</span>
title(<span class="string">'Default pipeline'</span>)
</pre><img vspace="5" hspace="5" src="L3demo_24.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
% The following script explains how the L^3 pipeline is applied to a RAW
% image from a sensor with an arbitrary CFA pattern.  

% The results of the training process, such as filters, are shown.  The
% calculations required to perform the training are not described but can
% be viewed in L3Train.m.

% The following code is taken from s_L3render and the functions that are
% subsequently called.  There have been slight modifications as needed
% and the plotting code has been added.  Indentations correspond with which
% m files the code was taken from.

%% Train an L^3 camera
s_L3TrainCamera
clearvars ** -except camera  %clear workspace except for camera

%% Load scene to capture with camera
scene = sceneFromFile('StuffedAnimals_tungsten-hdrs','multispectral');
meanLuminance = 600;
fovScene      = 10;
scene = sceneSet(scene,'hfov',fovScene);
scene = sceneAdjustLuminance(scene,meanLuminance);

% Adjust FOV of camera to match scene
camera = cameraSet(camera,'sensor fov',fovScene);

%% Perform optics and sensor simulation
% Following is in cameraCompute, which is called using the following line:
% camera = cameraCompute(camera,scene);

  oi     = cameraGet(camera,'oi');
  sensor = cameraGet(camera,'sensor');
  vci    = cameraGet(camera,'vci');  

  oi     = oiCompute(oi,scene);
  sensor = sensorCompute(sensor,oi);

%%  Move deeper in the code to more L^3 specific functions
% Following is in vcimageCompute, which is called using the followign line:

% vci    = vcimageCompute(vci,sensor);  
    L3 = imageGet(vci,'L3');

%   Following is in L3render, which is called using the following line:
    % [resultL3,lumIdx] = L3render(L3,sensor,'local');

%% Show CFA and RAW image
% The CFA pattern is visible in the RAW image.  Here the RAW image is
% drawn as a monochrome image although the measurement at each pixel
% has an associated color.
%
% The 2x2 RGBW CFA is used in this example.

      % RAW sensor image
      inputIm = sensorGet(sensor,'volts');
      vcNewGraphWin; imagesc(inputIm/max(inputIm(:))); colormap(gray)
      sz      = sensorGet(sensor,'size');
      title('RAW sensor image')
      
      % RGBW CFA
      L3plot(L3,'cfa full');       title('RGBW CFA')
 
      % Spectral sensitivities
      wave = sensorGet(sensor,'wave');
      sensitivities = sensorGet(sensor,'spectral QE');
      figure;   hold on
      plotcolors='rgbk';
      for colornum=1:4
          plot(wave,sensitivities(:,colornum),plotcolors(colornum))
      end
      xlabel('Wavelength (nm)');  title('Spectral Sensitivities')          
    
%% Show Patch Types
% L^3 is a patch based algorithm.  The output at each pixel is a
% function only of a set of nearby pixels in the input RAW image.
%
% Here a patch is a 5x5 square of pixels centered at the pixel where
% we want to calculate the output.  Since the CFA is a 2x2 pattern,
% there are 4 types of pixels (RGB and W), which could be at the
% center of a patch.  Therefore, we have the following four types of
% patches.
%
% Different filters have been trained for each patch type.  We will
% build the output image by iterating through each patch type and
% using the appropriate filters.

      warning('off') %#ok<*WNOFF>   %needed because of CFA/data mismatch      
      for patchtyperow=1:2
          for patchtypecol=1:2
              L3plot(L3,'block pattern',[patchtyperow,patchtypecol]);
              title(['Patch Type ',num2str(patchtyperow),', '...
                  ,num2str(patchtypecol)])
          end
      end
      warning('on') %#ok<*WNON>           
      
%% Load and show patches from RAW image
% All patches of a particular patch type are loaded at the same time.


      % For rest of this article, just consider patch type 1,1 that has a
      % center R pixel.  Normally all patch types are looped over.
      rr=1; cc=1;
      L3 = L3Set(L3,'patch type',[rr,cc]);
      
      % Load all patches of this cfa position from RAW image
      inputPatches = L3sensor2Patches(L3,inputIm); 
      
      % Patches are stored as a matrix where each vectorized patch is
      % stored in a row.
      
      % Size of this matrix: number of pixels in a patch x number of
      % R pixels in the RAW image.  (The border of the image is ignored
      % when a patch cannot fit so this number is slightly off.)      
      patchesSize = size(inputPatches)
      
      indices=ceil(rand(1,25)*size(inputPatches,2)/2); %randomly pick 25
      
      L3 = L3Set(L3,'sensor patches', inputPatches);
      L3plotpatches(L3,indices,5,5);
      title('Some of the Input Patches of Type 1,1')

%% Find patch luminance for each patch
% Since light level significantly alters the amount of noise in the
% measurements, the L^3 algorithm adapts to the local light level.
%
% The local light level for patch is called its patch luminance.  The patch
% luminance is a weighted average of the RAW voltage values and is
% calculated by:
%       1.  Finding the average color across the patch for each CFA color
%       2.  Averaging the numbers in (1).
% This calculation is performed by applying the luminance filter using an
% inner product.

%     Following is in L3applyPipeline2Patches, which is called using:
%      [xhatL3,lumIdx] = L3applyPipeline2Patches(L3,inputPatches,L3Type);

        luminancefilter = L3Get(L3,'luminance filter');
        L3plot(L3,'luminance filter');   title('Luminance Filter')
        
        allPatches = L3Get(L3,'sensor patches');
        patchluminances = luminancefilter*allPatches;
        
        figure;     hist(patchluminances,50)
        xlabel('Patch Luminance');      ylabel('Number of Patches')
        
%% Find closest patch luminance from trained set for each patch
% Filters are learned for a predefined set of patch luminance values.  For
% each patch, calculate the patch luminance and find the closest luminance
% value that was used for training.
% the patch

        %List of patch luminance values used for training
        patchLuminanceSamples = L3Get(L3,'luminance list')
        
        differences = repmat(patchluminances',1,length(patchLuminanceSamples)) - ...
            repmat(patchLuminanceSamples,length(patchluminances),1);
        [~,luminanceindex] = min(abs(differences'));

        figure;     hist(patchLuminanceSamples(luminanceindex),50)
        xlabel('Closest Patch Luminance');      ylabel('Number of Patches')

        % For rest of this article, just consider the patches that are
        % closest to the first trained patch luminance value.
        % Normally all patch types are looped over.
        ll = 1;
        currentpatches = find(luminanceindex == ll);
        
        %Set current patch luminance index
        L3 = L3Set(L3,'luminance type',ll);
        L3 = L3Set(L3,'sensor patches', allPatches(:,currentpatches));        
        
%% Global Linear Pipeline  (simpler alternative to full L^3 pipeline)
% The simplest way to run the pipeline is to have a single filter for each
% patch type and luminance level.  We call this the global linear pipeline
% (which is a bad name).  This pipeline is simpler than the full L^3
% pipelien and performance is usually only a little worse.

        L3plot(L3,'global filter');
        subplot(1,3,2)
        title('Global Pipeline Filters (3 plots are for XYZ)')

        %Output for global linear pipeline is calculated using a single
        %multiplication.
        globalpipelinefilter = L3Get(L3,'global filter');
        xhatL3(:,currentpatches) = globalpipelinefilter* L3Get(L3,'sensor patches');

%% Divide patches into flat and texture
% Patches are divided into two groups, flat and texture.
%
% Flat patches come from uniform regions of an image.  For a flat patch all
% measurements of the same color are nearly equal.
%
% Texture patches come from edges or texture regions of an imae.  For a
% texture patch, there is more variation across the patch measurements.

% Following are all implemented by repeated calls to L3Get.  Each of
% the following steps are calculated for each patch separately.
        
%%      1.  Calculate the mean in each color channel
            % means = L3Get(L3,'sensor patch means');
            meansFilter = L3Get(L3,'means filter');
            patches     = L3Get(L3,'sensor patches');
            means = meansFilter*patches;

            L3plot(L3,'mean filter');
            subplot(1,4,2)
            title('Color Channel Means Filters (4 plots are for RGBW)')
            
%%      2.  Subtract the mean from each pixel with the corresponding color         
            % patcheszeromean = L3Get(L3,'sensor patch zero mean')
            blockPattern = L3Get(L3,'block pattern');
            patcheszeromean = L3adjustpatchmean(patches,-means,blockPattern);
        
%%      3.  Find patch contrast by summing 0 mean patch with abs value        
            % contrasts = L3Get(L3,'sensor patch contrasts');
            contrasts = mean(abs(patcheszeromean));

%%      4.  Compare patch contrast to threshold.  If contrast<threshold, patch is flat.  If contrast>threshold, patch is texture.
            % flatindices = L3Get(L3,'flat indices');
            flatThreshold = L3Get(L3,'flat threshold');
            flatindices = (flatThreshold >= contrasts);

%% Apply filters to flat patches
% Optimal filters learned for the flat patches are applied to get the
% output XYZ estimates.
        flatfilters = L3Get(L3,'flat filters');
        xhatL3(:,currentpatches(flatindices)) = L3applyfilters(L3,flatfilters,flatindices);
                                
        L3plot(L3,'flat filter');
        subplot(1,3,2)        
        title('Filters for Flat Patches (3 plots are for XYZ)')

%% Flip texture patches into canonical form
% Since there are significant spatial differences within texture patches,
% it helps (a little) to decrease the possible variation in the patches.
% The patches are flipped over the vertical, horizontal, and main diagonal
% (assuming there is symmetry in the CFA pattern across these directions)
% so that each flipped texture patch has higher averages in the top, left,
% and above diagonal halves.

      textureindices = find(L3Get(L3,'texture indices'));
      randompick=ceil(rand(1,25)*size(textureindices,2)/2); %randomly pick 25
      selectedtextureindices = textureindices(randompick);
      
      L3plotpatches(L3,selectedtextureindices,5,5);
      title('Some Texture Patches before flip')

      L3 = L3flippatches(L3);
      
      L3plotpatches(L3,selectedtextureindices,5,5);
      title('Some Texture Patches after flip')      

%% Cluster texture patches
% The texture patches can be further subdivided using a hierarchical
% clustering method.  The idea is that by subdividing, similar patches will
% be grouped together and optimal filters for each cluster will be able to
% exploit the similar statistics of the patches.  This offers only small
% improvements for general natural scenes.
        L3 = L3clustertexturepatches(L3);        
                
%% Apply filters to texture patches
% Optimal filters learned for each cluster of texture patches are applied
% to get the output XYZ estimates.
        texturefilters = L3Get(L3,'texture filters');
        clustermembers = L3Get(L3,'cluster members');
        treedepth      = L3Get(L3,'tree depth');
        numclusters    = L3Get(L3,'nclusters');
        clusterrange   = 1:numclusters; %this should probably just be leaves
        for clusternum = clusterrange
            %clusterindices is vector of length equal to the number of allPatches,
            %each entry is 1 for each patch in the current cluster and 0 otherwise
            clusterindices = ...
                floor(clustermembers/2^(treedepth-floor(log2(clusternum))-1))==clusternum;

            xhatL3(:,currentpatches(clusterindices)) = ...
                L3applyfilters(L3,texturefilters{clusternum},clusterindices);
        end
                
        textureType = 1;
        L3Plot(L3,'texture filter',[rr,cc],ll,textureType);
        subplot(1,3,2)        
        title('Filters for all Texture Patches (3 plots are for XYZ)')
        
        textureType = 5;
        L3Plot(L3,'texture filter',[rr,cc],ll,textureType);
        subplot(1,3,2)        
        title('Filters for 1/4 of Texture Patches (3 plots are for XYZ)')        
                
%% Calculate final output images
% Once the above calculations are performed for each patch type and patch
% luminance (which happens by calling cameraCompute), we can get the final
% output images.
      
% Calculate L^3 result
[camera, lrgbL3] = cameraCompute(camera,scene);

% Calculate global L^3 result
camera  = cameraSet(camera,'vci name','L3 global');
[camera, lrgbGlobal] = cameraCompute(camera,'sensor');

%% Show which of the trained patch luminance values used at each pixel
vci = cameraGet(camera,'vci');
L3 = imageGet(vci,'L3');

lumIdx = L3Get(L3,'luminance index');
figure;  imagesc(lumIdx); axis image; axis off;
colorbar
title('Luminance Value Used')


%% Calculate ideal XYZ image
% Following XYZ image is the result that would occur with no noise if we
% had a sensor that measured the XYZ channels at every pixel.
[camera,xyzIdeal] = cameraCompute(camera,scene,'idealxyz');
xyzIdeal = xyzIdeal/max(xyzIdeal(:));   %scale to full display range

figure;  image(xyzIdeal); axis image; axis off;
title('Ideal XYZ')

%% Crop black border from all images
% L^3 does not give estimates for pixels near the image border.  An
% estimate is not possible for pixels when there is not enough room to fit 
% a patch.

xyzIdeal   = L3imcrop(L3,xyzIdeal);
lrgbL3      = L3imcrop(L3,lrgbL3);
lrgbGlobal  = L3imcrop(L3,lrgbGlobal);

%% Scale lrgb result images
% To achieve consistent appearance for display, the result images are
% scaled so they have the same mean as the lrgbIdeal image.  This is more
% consistent across the images than scaling each independently to achieve a
% maximum value of 1.

[srgbIdeal, lrgbIdeal] = xyz2srgb(xyzIdeal);
lrgbL3       = lrgbL3 * mean(lrgbIdeal(:)) / mean(lrgbL3(:));
lrgbGlobal   = lrgbGlobal * mean(lrgbIdeal(:)) / mean(lrgbGlobal(:));

%% Convert lrgb to srgb
srgbL3      = lrgb2srgb(ieClip(lrgbL3,0,1));
srgbGlobal  = lrgb2srgb(ieClip(lrgbGlobal,0,1));

%% Show the sRGB results
vcNewGraphWin;  imagesc(srgbIdeal); axis image
title('Ideal')

vcNewGraphWin; imagesc(srgbL3); axis image
title('L3')

vcNewGraphWin; imagesc(srgbGlobal); axis image
title('L3 Global')

%% Default ISET pipeline result
% Following is the default pipeline in ISET.  The settings and scaling are
% probably not set up properly.

camera = cameraSet(camera,'vci name','default');
[camera, lrgbbilinear] = cameraCompute(camera,'sensor');

%Crop image to compare with other cropped images
lrgbbilinear   = L3imcrop(L3,lrgbbilinear);

% Scale and convert to srgb
lrgbbilinear   = lrgbbilinear * mean(lrgbIdeal(:)) / mean(lrgbbilinear(:));
srgbbilinear   = lrgb2srgb(ieClip(lrgbbilinear,0,1));

vcNewGraphWin; imagesc(srgbbilinear); axis image
title('Default pipeline')
##### SOURCE END #####
--></body></html>